{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T17:38:51.422010Z",
     "start_time": "2020-06-03T17:38:44.639488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "RhcXTTUwEAwL",
    "outputId": "0ac4a477-f6d8-433f-874a-d04e5ba9e9b5"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from cifar import get_cifar10\n",
    "from misc import AverageMeter, accuracy\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gY5a73PN64s9"
   },
   "source": [
    "# Hyper-Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.501789Z",
     "start_time": "2020-05-29T12:00:53.536Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JXuYBWR964s-"
   },
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gpu_id      = 0        # id(s) for CUDA_VISIBLE_DEVICES\n",
    "        self.num_workers = 4        # number of workers\n",
    "        self.epochs      = 1024     # number of total epochs to run\n",
    "        self.start_epoch = 54       # manual epoch number (useful on restarts)\n",
    "        self.batch_size  = 64       # train batchsize\n",
    "        self.lr          = 0.03     # initial learning rate\n",
    "        self.warmup      = 0        # warmup epochs (unlabeled data based)\n",
    "        self.wdecay      = 5e-4     # weight decay\n",
    "        self.nesterov    = True     # use nesterov momentum\n",
    "        self.use_ema     = True     # use EMA model\n",
    "        self.ema_decay   = 0.999    # EMA decay rate\n",
    "        self.mu          = 7        # coefficient of unlabeled batch size\n",
    "        self.lambda_u    = 1        # coefficient of unlabeled loss\n",
    "        self.threshold   = 0.95     # pseudo label threshold\n",
    "        self.k_img       = 65536    # number of labeled examples to generate with augmentation techniques\n",
    "        self.out         = 'result' # directory to output the result\n",
    "        self.resume      = ''       # path to latest checkpoint (default: none)\n",
    "\n",
    "args = HyperParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyiTunUt64tG"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.504539Z",
     "start_time": "2020-05-29T12:00:53.542Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zkfupEId64tH"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(args, labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_model, scheduler, epoch):\n",
    "        \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    losses     = AverageMeter()\n",
    "    losses_x   = AverageMeter()\n",
    "    losses_u   = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    progress_bar = tqdm(range(args.iteration))\n",
    "    \n",
    "    train_loader = zip(labeled_trainloader, unlabeled_trainloader)\n",
    "    model.train()\n",
    "    for batch_idx, (data_x, data_u) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        # prepare data\n",
    "        inputs_x, targets_x         = data_x\n",
    "        (inputs_u_w, inputs_u_s), _ = data_u\n",
    "        batch_size                  = inputs_x.shape[0]\n",
    "        inputs                      = torch.cat((inputs_x, inputs_u_w, inputs_u_s)).to(args.device)\n",
    "\n",
    "        # Supervised Loss\n",
    "        targets_x                   = targets_x.to(args.device)\n",
    "        logits                      = model(inputs)\n",
    "        logits_x                    = logits[:batch_size]\n",
    "        logits_u_w, logits_u_s      = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "        Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "        # Unsupervised Loss\n",
    "        pseudo_label         = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
    "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "        mask                 = max_probs.ge(args.threshold).float()\n",
    "        Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "\n",
    "        # Final Loss\n",
    "        loss = Lx + args.lambda_u * Lu\n",
    "        loss.backward()\n",
    "\n",
    "        # update loss array\n",
    "        losses.update(loss.item())\n",
    "        losses_x.update(Lx.item())\n",
    "        losses_u.update(Lu.item())\n",
    "\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if args.use_ema:\n",
    "            ema_model.update(model)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # print progress\n",
    "        batch_time.update(time.time() - end)\n",
    "        end       = time.time()\n",
    "        mask_prob = mask.mean().item()\n",
    "        progress_bar.set_description(\"Train Epoch: {epoch}/{epochs:4}. Iter: {batch:4}/{iter:4}. LR: {lr:.6f}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. Loss_x: {loss_x:.4f}. Loss_u: {loss_u:.4f}. Mask: {mask:.4f}. \".format(\n",
    "                epoch=epoch + 1, epochs=args.epochs, batch=batch_idx + 1, iter=args.iteration,\n",
    "                lr=scheduler.get_last_lr()[0], data=data_time.avg, bt=batch_time.avg,\n",
    "                loss=losses.avg, loss_x=losses_x.avg, loss_u=losses_u.avg, mask=mask_prob))\n",
    "        progress_bar.update()\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    return losses.avg, losses_x.avg, losses_u.avg, mask_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvMGaPzT64tL"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.507468Z",
     "start_time": "2020-05-29T12:00:53.546Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tIpHPvcV64tM"
   },
   "outputs": [],
   "source": [
    "def test(args, test_loader, model, epoch):\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time  = AverageMeter()\n",
    "    losses     = AverageMeter()\n",
    "    top1       = AverageMeter()\n",
    "    top5       = AverageMeter()\n",
    "    end        = time.time()\n",
    "\n",
    "    test_loader = tqdm(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "            model.eval()\n",
    "\n",
    "            inputs  = inputs.to(args.device)\n",
    "            targets = targets.to(args.device)\n",
    "            outputs = model(inputs)\n",
    "            loss    = F.cross_entropy(outputs, targets)\n",
    "\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.shape[0])\n",
    "            top1.update(prec1.item(), inputs.shape[0])\n",
    "            top5.update(prec5.item(), inputs.shape[0])\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            \n",
    "            test_loader.set_description(\"Test Iter: {batch:4}/{iter:4}. Data: {data:.3f}s. Batch: {bt:.3f}s. Loss: {loss:.4f}. top1: {top1:.2f}. top5: {top5:.2f}. \".format(\n",
    "                    batch=batch_idx + 1, iter=len(test_loader), data=data_time.avg, bt=batch_time.avg,\n",
    "                    loss=losses.avg, top1=top1.avg, top5=top5.avg))\n",
    "        \n",
    "        test_loader.close()\n",
    "\n",
    "    logger.info(\"top-1 acc: {:.2f}\".format(top1.avg))\n",
    "    logger.info(\"top-5 acc: {:.2f}\".format(top5.avg))\n",
    "    return losses.avg, top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-cqvMxA64tP"
   },
   "source": [
    "# EMA (Exponential Moving Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.511375Z",
     "start_time": "2020-05-29T12:00:53.639Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "JwLjz8uB64tQ"
   },
   "outputs": [],
   "source": [
    "class ModelEMA(object):\n",
    "    def __init__(self, args, model, decay, device='', resume=''):\n",
    "        self.ema = deepcopy(model)\n",
    "        self.ema.eval()\n",
    "        self.decay  = decay\n",
    "        self.device = device\n",
    "        self.wd     = args.lr * args.wdecay\n",
    "        if device:\n",
    "            self.ema.to(device=device)\n",
    "        self.ema_has_module = hasattr(self.ema, 'module')\n",
    "        if resume:\n",
    "            self._load_checkpoint(resume)\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def _load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        assert isinstance(checkpoint, dict)\n",
    "        if 'ema_state_dict' in checkpoint:\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in checkpoint['ema_state_dict'].items():\n",
    "                if self.ema_has_module:\n",
    "                    name = 'module.' + k if not k.startswith('module') else k\n",
    "                else:\n",
    "                    name = k\n",
    "                new_state_dict[name] = v\n",
    "            self.ema.load_state_dict(new_state_dict)\n",
    "\n",
    "    def update(self, model):\n",
    "        needs_module = hasattr(model, 'module') and not self.ema_has_module\n",
    "        with torch.no_grad():\n",
    "            msd = model.state_dict()\n",
    "            for k, ema_v in self.ema.state_dict().items():\n",
    "                if needs_module:\n",
    "                    k = 'module.' + k\n",
    "                model_v = msd[k].detach()\n",
    "                if self.device:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n",
    "                # weight decay\n",
    "                if 'bn' not in k:\n",
    "                    msd[k] = msd[k] * (1. - self.wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1kGPxML7BJC"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=7./16., last_epoch=-1):\n",
    "    \n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "G20jRhax7BJH"
   },
   "source": [
    "## Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.521222Z",
     "start_time": "2020-05-29T12:00:53.859Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "vK6dDHmH7BJJ",
    "outputId": "787b7246-70b1-4d71-da9a-4c576f695722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2020 23:13:06 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "05/30/2020 23:13:06 - INFO - __main__ -   {'gpu_id': 0, 'num_workers': 4, 'epochs': 1024, 'start_epoch': 0, 'batch_size': 64, 'lr': 0.03, 'warmup': 0, 'wdecay': 0.0005, 'nesterov': True, 'use_ema': True, 'ema_decay': 0.999, 'mu': 7, 'lambda_u': 1, 'threshold': 0.95, 'k_img': 65536, 'out': 'result', 'resume': '', 'amp': True, 'opt_level': 'O1', 'local_rank': -1, 'device': device(type='cuda', index=0), 'world_size': 1, 'n_gpu': 1}\n"
     ]
    }
   ],
   "source": [
    "cudnn.benchmark = True\n",
    "global best_acc\n",
    "\n",
    "args.device     = torch.device('cuda', args.gpu_id)\n",
    "args.world_size = 1\n",
    "args.n_gpu      = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XT4DhdS77BJN"
   },
   "source": [
    "## Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.523176Z",
     "start_time": "2020-05-29T12:00:54.021Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "D1n2Ynr_7BJQ"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "os.makedirs(args.out, exist_ok=True)\n",
    "writer = SummaryWriter(args.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lCJ-OsLw7BJU"
   },
   "source": [
    "## Get CIFAR10 data (augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.524353Z",
     "start_time": "2020-05-29T12:00:54.200Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "89a8c7f7b2204c1abc4dca984ae9a554",
      "9f4140ed63f0401f88522b03dff1a1d5",
      "e1508db0d94b45718e593c1f85ed7284",
      "7c9d4fc4aa824d30b2a3fdf62b571b68",
      "1bcde06d6d254ff4bc17a7f149034526",
      "89bbc7041786423e9b6cd9b0a1d647f0",
      "cde28a322daf464797613453994a4215",
      "f6942dbb2bc147e19607ccdcf691abf5"
     ]
    },
    "colab_type": "code",
    "id": "9vGZ5qd27BJY",
    "outputId": "de3221ce-9648-4e69-989e-3f0af00bb728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a8c7f7b2204c1abc4dca984ae9a554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2020 23:13:25 - INFO - cifar -   Dataset: CIFAR10\n",
      "05/30/2020 23:13:25 - INFO - cifar -   Labeled examples: 65536 Unlabeled examples: 458752\n"
     ]
    }
   ],
   "source": [
    "num_labeled = 250\n",
    "labeled_dataset, unlabeled_dataset, test_dataset = get_cifar10('./data', num_labeled, args.k_img, args.k_img * args.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gp6MgH1e7BJa"
   },
   "source": [
    "## Create WideResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.528059Z",
     "start_time": "2020-05-29T12:00:54.330Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "J_yqTsXr7BJd",
    "outputId": "4d3f5526-bfbc-4909-b810-1fb4999306d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2020 23:13:25 - INFO - wideresnet -   Model: WideResNet 28x2\n",
      "05/30/2020 23:13:25 - INFO - __main__ -   Total params: 1.47M\n"
     ]
    }
   ],
   "source": [
    "from wideresnet import build_wideresnet\n",
    "model = build_wideresnet(depth=28, widen_factor=2, dropout=0, num_classes=10)\n",
    "logger.info(\"Total params: {:.2f}M\".format(sum(p.numel() for p in model.parameters())/1e6))\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "train_sampler = RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "VE10O9On7BJj"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.530983Z",
     "start_time": "2020-05-29T12:00:54.434Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5CCGx9Kw7BJn"
   },
   "outputs": [],
   "source": [
    "labeled_trainloader = DataLoader(\n",
    "        labeled_dataset,\n",
    "        sampler=train_sampler(labeled_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "unlabeled_trainloader = DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        sampler=train_sampler(unlabeled_dataset),\n",
    "        batch_size=args.batch_size*args.mu,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        sampler=SequentialSampler(test_dataset),\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "U92p51F-7BJs"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.532939Z",
     "start_time": "2020-05-29T12:00:54.561Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RjJ5c1eV7BJu"
   },
   "outputs": [],
   "source": [
    "optimizer        = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=args.nesterov)\n",
    "args.iteration   = args.k_img // args.batch_size // args.world_size\n",
    "args.total_steps = args.epochs * args.iteration\n",
    "scheduler        = get_cosine_schedule_with_warmup(optimizer, args.warmup * args.iteration, args.total_steps)\n",
    "\n",
    "if args.use_ema:\n",
    "    ema_model = ModelEMA(args, model, args.ema_decay, args.device)\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yTnNC2DY7BJw"
   },
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.535864Z",
     "start_time": "2020-05-29T12:00:54.678Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "3UZhA5r47BJz",
    "outputId": "37ec96c2-c9b4-43ae-927e-86d2e63190a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/30/2020 23:13:41 - INFO - __main__ -   ***** Running training *****\n",
      "05/30/2020 23:13:41 - INFO - __main__ -     Task = cifar10@250\n",
      "05/30/2020 23:13:41 - INFO - __main__ -     Num Epochs = 1024\n",
      "05/30/2020 23:13:41 - INFO - __main__ -     Batch size per GPU = 64\n",
      "05/30/2020 23:13:41 - INFO - __main__ -     Total train batch size = 64\n",
      "05/30/2020 23:13:41 - INFO - __main__ -     Total optimization steps = 1048576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
     ]
    }
   ],
   "source": [
    "if args.resume:\n",
    "    logger.info(\"==> Resuming from checkpoint..\")\n",
    "    assert os.path.isfile(args.resume), \"Error: no checkpoint directory found!\"\n",
    "    args.out    = os.path.dirname(args.resume)\n",
    "    checkpoint  = torch.load(args.resume)\n",
    "    best_acc    = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if args.use_ema:\n",
    "        ema_model.ema.load_state_dict(checkpoint['ema_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Task = {'cifar10'}@{'250'}\")\n",
    "logger.info(f\"  Num Epochs = {args.epochs}\")\n",
    "logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
    "logger.info(f\"  Total train batch size = {args.batch_size*args.world_size}\")\n",
    "logger.info(f\"  Total optimization steps = {args.total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T18:57:18.832695Z",
     "start_time": "2020-05-22T18:57:13.034767Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0Jstmh327BJ3"
   },
   "source": [
    "## Loop on Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:01:04.538233Z",
     "start_time": "2020-05-29T12:00:54.810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0TtW15jR7BJ6",
    "outputId": "5ad92723-b15d-4d55-be55-d647bb3897c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Train Epoch: 1/1024. Iter: 1024/1024. LR: 0.030000. Data: 0.028s. Batch: 0.609s. Loss: 0.5294. Loss_x: 0.2882. Loss_u: 0.2411. Mask: 0.4509. : 100%|██████████| 1024/1024 [10:23<00:00,  1.64it/s]\n",
      "05/30/2020 23:24:04 - INFO - __main__ -   Epoch 1. train_loss: 0.5294. train_loss_x: 0.2882. train_loss_u: 0.2411.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 9.3724. top1: 18.92. top5: 54.28. : 100%|██████████| 157/157 [00:06<00:00, 25.14it/s]\n",
      "05/30/2020 23:24:11 - INFO - __main__ -   top-1 acc: 18.92\n",
      "05/30/2020 23:24:11 - INFO - __main__ -   top-5 acc: 54.28\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "05/30/2020 23:24:11 - INFO - __main__ -   Best top-1 acc: 18.92\n",
      "05/30/2020 23:24:11 - INFO - __main__ -   Mean top-1 acc: 18.92\n",
      "\n",
      "Train Epoch: 2/1024. Iter: 1024/1024. LR: 0.030000. Data: 0.030s. Batch: 0.618s. Loss: 0.3234. Loss_x: 0.0194. Loss_u: 0.3040. Mask: 0.4732. : 100%|██████████| 1024/1024 [10:32<00:00,  1.62it/s]\n",
      "05/30/2020 23:34:44 - INFO - __main__ -   Epoch 2. train_loss: 0.3234. train_loss_x: 0.0194. train_loss_u: 0.3040.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 4.0216. top1: 37.06. top5: 80.30. : 100%|██████████| 157/157 [00:06<00:00, 26.12it/s]\n",
      "05/30/2020 23:34:50 - INFO - __main__ -   top-1 acc: 37.06\n",
      "05/30/2020 23:34:50 - INFO - __main__ -   top-5 acc: 80.30\n",
      "05/30/2020 23:34:50 - INFO - __main__ -   Best top-1 acc: 37.06\n",
      "05/30/2020 23:34:50 - INFO - __main__ -   Mean top-1 acc: 27.99\n",
      "\n",
      "Train Epoch: 3/1024. Iter: 1024/1024. LR: 0.030000. Data: 0.029s. Batch: 0.612s. Loss: 0.2994. Loss_x: 0.0103. Loss_u: 0.2891. Mask: 0.5491. : 100%|██████████| 1024/1024 [10:27<00:00,  1.63it/s]\n",
      "05/30/2020 23:45:17 - INFO - __main__ -   Epoch 3. train_loss: 0.2994. train_loss_x: 0.0103. train_loss_u: 0.2891.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.036s. Loss: 3.2002. top1: 50.18. top5: 88.50. : 100%|██████████| 157/157 [00:05<00:00, 27.05it/s]\n",
      "05/30/2020 23:45:23 - INFO - __main__ -   top-1 acc: 50.18\n",
      "05/30/2020 23:45:23 - INFO - __main__ -   top-5 acc: 88.50\n",
      "05/30/2020 23:45:23 - INFO - __main__ -   Best top-1 acc: 50.18\n",
      "05/30/2020 23:45:23 - INFO - __main__ -   Mean top-1 acc: 35.39\n",
      "\n",
      "Train Epoch: 4/1024. Iter: 1024/1024. LR: 0.030000. Data: 0.028s. Batch: 0.601s. Loss: 0.2763. Loss_x: 0.0064. Loss_u: 0.2699. Mask: 0.5536. : 100%|██████████| 1024/1024 [10:15<00:00,  1.66it/s]\n",
      "05/30/2020 23:55:38 - INFO - __main__ -   Epoch 4. train_loss: 0.2763. train_loss_x: 0.0064. train_loss_u: 0.2699.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.036s. Loss: 2.6145. top1: 57.37. top5: 92.50. : 100%|██████████| 157/157 [00:05<00:00, 27.07it/s]\n",
      "05/30/2020 23:55:44 - INFO - __main__ -   top-1 acc: 57.37\n",
      "05/30/2020 23:55:44 - INFO - __main__ -   top-5 acc: 92.50\n",
      "05/30/2020 23:55:44 - INFO - __main__ -   Best top-1 acc: 57.37\n",
      "05/30/2020 23:55:44 - INFO - __main__ -   Mean top-1 acc: 40.88\n",
      "\n",
      "Train Epoch: 5/1024. Iter:  250/1024. LR: 0.030000. Data: 0.035s. Batch: 0.615s. Loss: 0.2673. Loss_x: 0.0059. Loss_u: 0.2614. Mask: 0.5580. :  24%|██▍       | 250/1024 [02:33<07:15,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 131072.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5/1024. Iter: 1024/1024. LR: 0.029999. Data: 0.028s. Batch: 0.601s. Loss: 0.2640. Loss_x: 0.0052. Loss_u: 0.2588. Mask: 0.6272. : 100%|██████████| 1024/1024 [10:15<00:00,  1.66it/s]\n",
      "05/31/2020 00:05:59 - INFO - __main__ -   Epoch 5. train_loss: 0.2640. train_loss_x: 0.0052. train_loss_u: 0.2588.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 2.3510. top1: 62.13. top5: 94.45. : 100%|██████████| 157/157 [00:05<00:00, 26.57it/s]\n",
      "05/31/2020 00:06:05 - INFO - __main__ -   top-1 acc: 62.13\n",
      "05/31/2020 00:06:05 - INFO - __main__ -   top-5 acc: 94.45\n",
      "05/31/2020 00:06:05 - INFO - __main__ -   Best top-1 acc: 62.13\n",
      "05/31/2020 00:06:05 - INFO - __main__ -   Mean top-1 acc: 45.13\n",
      "\n",
      "Train Epoch: 6/1024. Iter: 1024/1024. LR: 0.029999. Data: 0.028s. Batch: 0.603s. Loss: 0.2555. Loss_x: 0.0039. Loss_u: 0.2516. Mask: 0.6518. : 100%|██████████| 1024/1024 [10:17<00:00,  1.66it/s]\n",
      "05/31/2020 00:16:23 - INFO - __main__ -   Epoch 6. train_loss: 0.2555. train_loss_x: 0.0039. train_loss_u: 0.2516.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.036s. Loss: 2.2206. top1: 64.85. top5: 95.37. : 100%|██████████| 157/157 [00:05<00:00, 27.25it/s]\n",
      "05/31/2020 00:16:29 - INFO - __main__ -   top-1 acc: 64.85\n",
      "05/31/2020 00:16:29 - INFO - __main__ -   top-5 acc: 95.37\n",
      "05/31/2020 00:16:29 - INFO - __main__ -   Best top-1 acc: 64.85\n",
      "05/31/2020 00:16:29 - INFO - __main__ -   Mean top-1 acc: 48.42\n",
      "\n",
      "Train Epoch: 7/1024. Iter: 1024/1024. LR: 0.029999. Data: 0.028s. Batch: 0.605s. Loss: 0.2471. Loss_x: 0.0033. Loss_u: 0.2439. Mask: 0.6562. : 100%|██████████| 1024/1024 [10:19<00:00,  1.65it/s]\n",
      "05/31/2020 00:26:48 - INFO - __main__ -   Epoch 7. train_loss: 0.2471. train_loss_x: 0.0033. train_loss_u: 0.2439.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 2.0108. top1: 68.03. top5: 95.82. : 100%|██████████| 157/157 [00:05<00:00, 26.83it/s]\n",
      "05/31/2020 00:26:54 - INFO - __main__ -   top-1 acc: 68.03\n",
      "05/31/2020 00:26:54 - INFO - __main__ -   top-5 acc: 95.82\n",
      "05/31/2020 00:26:54 - INFO - __main__ -   Best top-1 acc: 68.03\n",
      "05/31/2020 00:26:54 - INFO - __main__ -   Mean top-1 acc: 51.22\n",
      "\n",
      "Train Epoch: 8/1024. Iter: 1024/1024. LR: 0.029998. Data: 0.030s. Batch: 0.615s. Loss: 0.2407. Loss_x: 0.0025. Loss_u: 0.2381. Mask: 0.6741. : 100%|██████████| 1024/1024 [10:30<00:00,  1.62it/s]\n",
      "05/31/2020 00:37:24 - INFO - __main__ -   Epoch 8. train_loss: 0.2407. train_loss_x: 0.0025. train_loss_u: 0.2381.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 1.8877. top1: 70.54. top5: 96.43. : 100%|██████████| 157/157 [00:06<00:00, 25.16it/s]\n",
      "05/31/2020 00:37:31 - INFO - __main__ -   top-1 acc: 70.54\n",
      "05/31/2020 00:37:31 - INFO - __main__ -   top-5 acc: 96.43\n",
      "05/31/2020 00:37:31 - INFO - __main__ -   Best top-1 acc: 70.54\n",
      "05/31/2020 00:37:31 - INFO - __main__ -   Mean top-1 acc: 53.63\n",
      "\n",
      "Train Epoch: 9/1024. Iter:  364/1024. LR: 0.029998. Data: 0.035s. Batch: 0.639s. Loss: 0.2367. Loss_x: 0.0020. Loss_u: 0.2347. Mask: 0.6696. :  36%|███▌      | 364/1024 [03:52<06:29,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/1024. Iter: 1024/1024. LR: 0.029998. Data: 0.032s. Batch: 0.645s. Loss: 0.2347. Loss_x: 0.0021. Loss_u: 0.2326. Mask: 0.7321. : 100%|██████████| 1024/1024 [11:01<00:00,  1.55it/s]\n",
      "05/31/2020 00:48:32 - INFO - __main__ -   Epoch 9. train_loss: 0.2347. train_loss_x: 0.0021. train_loss_u: 0.2326.\n",
      "Test Iter:  157/ 157. Data: 0.010s. Batch: 0.044s. Loss: 1.7659. top1: 72.29. top5: 96.88. : 100%|██████████| 157/157 [00:06<00:00, 22.59it/s]\n",
      "05/31/2020 00:48:39 - INFO - __main__ -   top-1 acc: 72.29\n",
      "05/31/2020 00:48:39 - INFO - __main__ -   top-5 acc: 96.88\n",
      "05/31/2020 00:48:39 - INFO - __main__ -   Best top-1 acc: 72.29\n",
      "05/31/2020 00:48:39 - INFO - __main__ -   Mean top-1 acc: 55.71\n",
      "\n",
      "Train Epoch: 10/1024. Iter: 1024/1024. LR: 0.029997. Data: 0.033s. Batch: 0.658s. Loss: 0.2304. Loss_x: 0.0017. Loss_u: 0.2287. Mask: 0.7054. : 100%|██████████| 1024/1024 [11:13<00:00,  1.52it/s]\n",
      "05/31/2020 00:59:53 - INFO - __main__ -   Epoch 10. train_loss: 0.2304. train_loss_x: 0.0017. train_loss_u: 0.2287.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 1.7153. top1: 73.70. top5: 97.15. : 100%|██████████| 157/157 [00:06<00:00, 25.16it/s]\n",
      "05/31/2020 00:59:59 - INFO - __main__ -   top-1 acc: 73.70\n",
      "05/31/2020 00:59:59 - INFO - __main__ -   top-5 acc: 97.15\n",
      "05/31/2020 00:59:59 - INFO - __main__ -   Best top-1 acc: 73.70\n",
      "05/31/2020 00:59:59 - INFO - __main__ -   Mean top-1 acc: 57.51\n",
      "\n",
      "Train Epoch: 11/1024. Iter: 1024/1024. LR: 0.029997. Data: 0.031s. Batch: 0.637s. Loss: 0.2274. Loss_x: 0.0016. Loss_u: 0.2258. Mask: 0.7455. : 100%|██████████| 1024/1024 [10:51<00:00,  1.57it/s]\n",
      "05/31/2020 01:10:51 - INFO - __main__ -   Epoch 11. train_loss: 0.2274. train_loss_x: 0.0016. train_loss_u: 0.2258.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 1.5912. top1: 75.20. top5: 97.39. : 100%|██████████| 157/157 [00:06<00:00, 25.19it/s]\n",
      "05/31/2020 01:10:57 - INFO - __main__ -   top-1 acc: 75.20\n",
      "05/31/2020 01:10:57 - INFO - __main__ -   top-5 acc: 97.39\n",
      "05/31/2020 01:10:57 - INFO - __main__ -   Best top-1 acc: 75.20\n",
      "05/31/2020 01:10:57 - INFO - __main__ -   Mean top-1 acc: 59.12\n",
      "\n",
      "Train Epoch: 12/1024. Iter:  218/1024. LR: 0.029997. Data: 0.039s. Batch: 0.645s. Loss: 0.2278. Loss_x: 0.0015. Loss_u: 0.2262. Mask: 0.7031. :  21%|██▏       | 218/1024 [02:20<08:05,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12/1024. Iter: 1024/1024. LR: 0.029996. Data: 0.029s. Batch: 0.619s. Loss: 0.2251. Loss_x: 0.0014. Loss_u: 0.2238. Mask: 0.7991. : 100%|██████████| 1024/1024 [10:34<00:00,  1.61it/s]\n",
      "05/31/2020 01:21:32 - INFO - __main__ -   Epoch 12. train_loss: 0.2251. train_loss_x: 0.0014. train_loss_u: 0.2238.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 1.5304. top1: 75.52. top5: 97.49. : 100%|██████████| 157/157 [00:05<00:00, 26.27it/s]\n",
      "05/31/2020 01:21:38 - INFO - __main__ -   top-1 acc: 75.52\n",
      "05/31/2020 01:21:38 - INFO - __main__ -   top-5 acc: 97.49\n",
      "05/31/2020 01:21:38 - INFO - __main__ -   Best top-1 acc: 75.52\n",
      "05/31/2020 01:21:38 - INFO - __main__ -   Mean top-1 acc: 60.48\n",
      "\n",
      "Train Epoch: 13/1024. Iter: 1024/1024. LR: 0.029995. Data: 0.029s. Batch: 0.608s. Loss: 0.2207. Loss_x: 0.0014. Loss_u: 0.2193. Mask: 0.7455. : 100%|██████████| 1024/1024 [10:22<00:00,  1.64it/s]\n",
      "05/31/2020 01:32:00 - INFO - __main__ -   Epoch 13. train_loss: 0.2207. train_loss_x: 0.0014. train_loss_u: 0.2193.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 1.4504. top1: 76.74. top5: 97.52. : 100%|██████████| 157/157 [00:05<00:00, 26.48it/s]\n",
      "05/31/2020 01:32:06 - INFO - __main__ -   top-1 acc: 76.74\n",
      "05/31/2020 01:32:06 - INFO - __main__ -   top-5 acc: 97.52\n",
      "05/31/2020 01:32:06 - INFO - __main__ -   Best top-1 acc: 76.74\n",
      "05/31/2020 01:32:06 - INFO - __main__ -   Mean top-1 acc: 61.73\n",
      "\n",
      "Train Epoch: 14/1024. Iter:  734/1024. LR: 0.029995. Data: 0.030s. Batch: 0.609s. Loss: 0.2200. Loss_x: 0.0011. Loss_u: 0.2189. Mask: 0.7411. :  72%|███████▏  | 734/1024 [07:26<02:47,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14/1024. Iter: 1024/1024. LR: 0.029995. Data: 0.029s. Batch: 0.606s. Loss: 0.2195. Loss_x: 0.0011. Loss_u: 0.2185. Mask: 0.7656. : 100%|██████████| 1024/1024 [10:20<00:00,  1.65it/s]\n",
      "05/31/2020 01:42:27 - INFO - __main__ -   Epoch 14. train_loss: 0.2195. train_loss_x: 0.0011. train_loss_u: 0.2185.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 1.3970. top1: 77.67. top5: 97.68. : 100%|██████████| 157/157 [00:05<00:00, 26.46it/s]\n",
      "05/31/2020 01:42:33 - INFO - __main__ -   top-1 acc: 77.67\n",
      "05/31/2020 01:42:33 - INFO - __main__ -   top-5 acc: 97.68\n",
      "05/31/2020 01:42:33 - INFO - __main__ -   Best top-1 acc: 77.67\n",
      "05/31/2020 01:42:33 - INFO - __main__ -   Mean top-1 acc: 62.87\n",
      "\n",
      "Train Epoch: 15/1024. Iter: 1024/1024. LR: 0.029994. Data: 0.029s. Batch: 0.606s. Loss: 0.2151. Loss_x: 0.0011. Loss_u: 0.2140. Mask: 0.7500. : 100%|██████████| 1024/1024 [10:20<00:00,  1.65it/s]\n",
      "05/31/2020 01:52:54 - INFO - __main__ -   Epoch 15. train_loss: 0.2151. train_loss_x: 0.0011. train_loss_u: 0.2140.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.036s. Loss: 1.3270. top1: 78.32. top5: 97.84. : 100%|██████████| 157/157 [00:05<00:00, 27.05it/s]\n",
      "05/31/2020 01:53:00 - INFO - __main__ -   top-1 acc: 78.32\n",
      "05/31/2020 01:53:00 - INFO - __main__ -   top-5 acc: 97.84\n",
      "05/31/2020 01:53:00 - INFO - __main__ -   Best top-1 acc: 78.32\n",
      "05/31/2020 01:53:00 - INFO - __main__ -   Mean top-1 acc: 63.90\n",
      "\n",
      "Train Epoch: 16/1024. Iter: 1024/1024. LR: 0.029993. Data: 0.030s. Batch: 0.611s. Loss: 0.2127. Loss_x: 0.0008. Loss_u: 0.2119. Mask: 0.8103. : 100%|██████████| 1024/1024 [10:25<00:00,  1.64it/s]\n",
      "05/31/2020 02:03:26 - INFO - __main__ -   Epoch 16. train_loss: 0.2127. train_loss_x: 0.0008. train_loss_u: 0.2119.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 1.2693. top1: 79.18. top5: 97.90. : 100%|██████████| 157/157 [00:05<00:00, 26.72it/s]\n",
      "05/31/2020 02:03:32 - INFO - __main__ -   top-1 acc: 79.18\n",
      "05/31/2020 02:03:32 - INFO - __main__ -   top-5 acc: 97.90\n",
      "05/31/2020 02:03:32 - INFO - __main__ -   Best top-1 acc: 79.18\n",
      "05/31/2020 02:03:32 - INFO - __main__ -   Mean top-1 acc: 64.86\n",
      "\n",
      "Train Epoch: 17/1024. Iter: 1024/1024. LR: 0.029992. Data: 0.030s. Batch: 0.626s. Loss: 0.2104. Loss_x: 0.0008. Loss_u: 0.2096. Mask: 0.8304. : 100%|██████████| 1024/1024 [10:41<00:00,  1.60it/s]\n",
      "05/31/2020 02:14:13 - INFO - __main__ -   Epoch 17. train_loss: 0.2104. train_loss_x: 0.0008. train_loss_u: 0.2096.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 1.2885. top1: 79.46. top5: 98.13. : 100%|██████████| 157/157 [00:06<00:00, 24.76it/s]\n",
      "05/31/2020 02:14:20 - INFO - __main__ -   top-1 acc: 79.46\n",
      "05/31/2020 02:14:20 - INFO - __main__ -   top-5 acc: 98.13\n",
      "05/31/2020 02:14:20 - INFO - __main__ -   Best top-1 acc: 79.46\n",
      "05/31/2020 02:14:20 - INFO - __main__ -   Mean top-1 acc: 65.72\n",
      "\n",
      "Train Epoch: 18/1024. Iter:  918/1024. LR: 0.029991. Data: 0.031s. Batch: 0.642s. Loss: 0.2093. Loss_x: 0.0008. Loss_u: 0.2086. Mask: 0.7991. :  90%|████████▉ | 918/1024 [09:49<01:05,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18/1024. Iter: 1024/1024. LR: 0.029991. Data: 0.031s. Batch: 0.640s. Loss: 0.2092. Loss_x: 0.0008. Loss_u: 0.2085. Mask: 0.7746. : 100%|██████████| 1024/1024 [10:55<00:00,  1.56it/s]\n",
      "05/31/2020 02:25:15 - INFO - __main__ -   Epoch 18. train_loss: 0.2092. train_loss_x: 0.0008. train_loss_u: 0.2085.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 1.2806. top1: 79.57. top5: 98.12. : 100%|██████████| 157/157 [00:06<00:00, 24.74it/s]\n",
      "05/31/2020 02:25:22 - INFO - __main__ -   top-1 acc: 79.57\n",
      "05/31/2020 02:25:22 - INFO - __main__ -   top-5 acc: 98.12\n",
      "05/31/2020 02:25:22 - INFO - __main__ -   Best top-1 acc: 79.57\n",
      "05/31/2020 02:25:22 - INFO - __main__ -   Mean top-1 acc: 66.48\n",
      "\n",
      "Train Epoch: 19/1024. Iter: 1024/1024. LR: 0.029990. Data: 0.031s. Batch: 0.643s. Loss: 0.2083. Loss_x: 0.0007. Loss_u: 0.2076. Mask: 0.7723. : 100%|██████████| 1024/1024 [10:58<00:00,  1.55it/s]\n",
      "05/31/2020 02:36:20 - INFO - __main__ -   Epoch 19. train_loss: 0.2083. train_loss_x: 0.0007. train_loss_u: 0.2076.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 1.2101. top1: 80.42. top5: 98.20. : 100%|██████████| 157/157 [00:06<00:00, 24.69it/s]\n",
      "05/31/2020 02:36:27 - INFO - __main__ -   top-1 acc: 80.42\n",
      "05/31/2020 02:36:27 - INFO - __main__ -   top-5 acc: 98.20\n",
      "05/31/2020 02:36:27 - INFO - __main__ -   Best top-1 acc: 80.42\n",
      "05/31/2020 02:36:27 - INFO - __main__ -   Mean top-1 acc: 67.22\n",
      "\n",
      "Train Epoch: 20/1024. Iter: 1024/1024. LR: 0.029989. Data: 0.032s. Batch: 0.643s. Loss: 0.2070. Loss_x: 0.0006. Loss_u: 0.2064. Mask: 0.8259. : 100%|██████████| 1024/1024 [10:59<00:00,  1.55it/s]\n",
      "05/31/2020 02:47:26 - INFO - __main__ -   Epoch 20. train_loss: 0.2070. train_loss_x: 0.0006. train_loss_u: 0.2064.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 1.1073. top1: 81.28. top5: 98.34. : 100%|██████████| 157/157 [00:06<00:00, 24.90it/s]\n",
      "05/31/2020 02:47:32 - INFO - __main__ -   top-1 acc: 81.28\n",
      "05/31/2020 02:47:32 - INFO - __main__ -   top-5 acc: 98.34\n",
      "05/31/2020 02:47:32 - INFO - __main__ -   Best top-1 acc: 81.28\n",
      "05/31/2020 02:47:32 - INFO - __main__ -   Mean top-1 acc: 67.92\n",
      "\n",
      "Train Epoch: 21/1024. Iter:  299/1024. LR: 0.029989. Data: 0.038s. Batch: 0.655s. Loss: 0.2060. Loss_x: 0.0005. Loss_u: 0.2055. Mask: 0.8237. :  29%|██▉       | 299/1024 [03:15<07:18,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21/1024. Iter: 1024/1024. LR: 0.029988. Data: 0.032s. Batch: 0.644s. Loss: 0.2049. Loss_x: 0.0005. Loss_u: 0.2044. Mask: 0.8304. : 100%|██████████| 1024/1024 [10:59<00:00,  1.55it/s]\n",
      "05/31/2020 02:58:32 - INFO - __main__ -   Epoch 21. train_loss: 0.2049. train_loss_x: 0.0005. train_loss_u: 0.2044.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.039s. Loss: 1.0937. top1: 81.69. top5: 98.39. : 100%|██████████| 157/157 [00:06<00:00, 25.30it/s]\n",
      "05/31/2020 02:58:39 - INFO - __main__ -   top-1 acc: 81.69\n",
      "05/31/2020 02:58:39 - INFO - __main__ -   top-5 acc: 98.39\n",
      "05/31/2020 02:58:39 - INFO - __main__ -   Best top-1 acc: 81.69\n",
      "05/31/2020 02:58:39 - INFO - __main__ -   Mean top-1 acc: 71.06\n",
      "\n",
      "Train Epoch: 22/1024. Iter: 1024/1024. LR: 0.029987. Data: 0.032s. Batch: 0.643s. Loss: 0.2047. Loss_x: 0.0006. Loss_u: 0.2041. Mask: 0.8125. : 100%|██████████| 1024/1024 [10:58<00:00,  1.56it/s]\n",
      "05/31/2020 03:09:37 - INFO - __main__ -   Epoch 22. train_loss: 0.2047. train_loss_x: 0.0006. train_loss_u: 0.2041.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 1.0557. top1: 82.30. top5: 98.50. : 100%|██████████| 157/157 [00:06<00:00, 24.86it/s]\n",
      "05/31/2020 03:09:43 - INFO - __main__ -   top-1 acc: 82.30\n",
      "05/31/2020 03:09:43 - INFO - __main__ -   top-5 acc: 98.50\n",
      "05/31/2020 03:09:43 - INFO - __main__ -   Best top-1 acc: 82.30\n",
      "05/31/2020 03:09:43 - INFO - __main__ -   Mean top-1 acc: 73.32\n",
      "\n",
      "Train Epoch: 23/1024. Iter:  934/1024. LR: 0.029986. Data: 0.031s. Batch: 0.640s. Loss: 0.2031. Loss_x: 0.0005. Loss_u: 0.2026. Mask: 0.7946. :  91%|█████████ | 934/1024 [09:57<00:55,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 23/1024. Iter: 1024/1024. LR: 0.029986. Data: 0.030s. Batch: 0.638s. Loss: 0.2033. Loss_x: 0.0005. Loss_u: 0.2028. Mask: 0.8259. : 100%|██████████| 1024/1024 [10:53<00:00,  1.57it/s]\n",
      "05/31/2020 03:20:37 - INFO - __main__ -   Epoch 23. train_loss: 0.2033. train_loss_x: 0.0005. train_loss_u: 0.2028.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 1.0131. top1: 82.76. top5: 98.59. : 100%|██████████| 157/157 [00:06<00:00, 25.21it/s]\n",
      "05/31/2020 03:20:44 - INFO - __main__ -   top-1 acc: 82.76\n",
      "05/31/2020 03:20:44 - INFO - __main__ -   top-5 acc: 98.59\n",
      "05/31/2020 03:20:44 - INFO - __main__ -   Best top-1 acc: 82.76\n",
      "05/31/2020 03:20:44 - INFO - __main__ -   Mean top-1 acc: 74.95\n",
      "\n",
      "Train Epoch: 24/1024. Iter: 1024/1024. LR: 0.029984. Data: 0.031s. Batch: 0.640s. Loss: 0.2011. Loss_x: 0.0005. Loss_u: 0.2006. Mask: 0.8304. : 100%|██████████| 1024/1024 [10:55<00:00,  1.56it/s]\n",
      "05/31/2020 03:31:40 - INFO - __main__ -   Epoch 24. train_loss: 0.2011. train_loss_x: 0.0005. train_loss_u: 0.2006.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 1.0028. top1: 83.19. top5: 98.63. : 100%|██████████| 157/157 [00:06<00:00, 25.22it/s]\n",
      "05/31/2020 03:31:46 - INFO - __main__ -   top-1 acc: 83.19\n",
      "05/31/2020 03:31:46 - INFO - __main__ -   top-5 acc: 98.63\n",
      "05/31/2020 03:31:46 - INFO - __main__ -   Best top-1 acc: 83.19\n",
      "05/31/2020 03:31:46 - INFO - __main__ -   Mean top-1 acc: 76.24\n",
      "\n",
      "Train Epoch: 25/1024. Iter:  936/1024. LR: 0.029983. Data: 0.032s. Batch: 0.643s. Loss: 0.2020. Loss_x: 0.0005. Loss_u: 0.2015. Mask: 0.8616. :  91%|█████████▏| 936/1024 [10:02<00:53,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25/1024. Iter: 1024/1024. LR: 0.029983. Data: 0.032s. Batch: 0.641s. Loss: 0.2016. Loss_x: 0.0005. Loss_u: 0.2011. Mask: 0.8259. : 100%|██████████| 1024/1024 [10:56<00:00,  1.56it/s]\n",
      "05/31/2020 03:42:43 - INFO - __main__ -   Epoch 25. train_loss: 0.2016. train_loss_x: 0.0005. train_loss_u: 0.2011.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 0.9884. top1: 83.45. top5: 98.63. : 100%|██████████| 157/157 [00:06<00:00, 24.66it/s]\n",
      "05/31/2020 03:42:49 - INFO - __main__ -   top-1 acc: 83.45\n",
      "05/31/2020 03:42:49 - INFO - __main__ -   top-5 acc: 98.63\n",
      "05/31/2020 03:42:49 - INFO - __main__ -   Best top-1 acc: 83.45\n",
      "05/31/2020 03:42:49 - INFO - __main__ -   Mean top-1 acc: 77.31\n",
      "\n",
      "Train Epoch: 26/1024. Iter: 1024/1024. LR: 0.029982. Data: 0.033s. Batch: 0.641s. Loss: 0.1999. Loss_x: 0.0004. Loss_u: 0.1995. Mask: 0.8237. : 100%|██████████| 1024/1024 [10:56<00:00,  1.56it/s]\n",
      "05/31/2020 03:53:46 - INFO - __main__ -   Epoch 26. train_loss: 0.1999. train_loss_x: 0.0004. train_loss_u: 0.1995.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 0.9679. top1: 83.86. top5: 98.68. : 100%|██████████| 157/157 [00:06<00:00, 24.72it/s]\n",
      "05/31/2020 03:53:52 - INFO - __main__ -   top-1 acc: 83.86\n",
      "05/31/2020 03:53:52 - INFO - __main__ -   top-5 acc: 98.68\n",
      "05/31/2020 03:53:53 - INFO - __main__ -   Best top-1 acc: 83.86\n",
      "05/31/2020 03:53:53 - INFO - __main__ -   Mean top-1 acc: 78.26\n",
      "\n",
      "Train Epoch: 27/1024. Iter: 1024/1024. LR: 0.029980. Data: 0.031s. Batch: 0.641s. Loss: 0.1987. Loss_x: 0.0004. Loss_u: 0.1983. Mask: 0.8549. : 100%|██████████| 1024/1024 [10:56<00:00,  1.56it/s]\n",
      "05/31/2020 04:04:49 - INFO - __main__ -   Epoch 27. train_loss: 0.1987. train_loss_x: 0.0004. train_loss_u: 0.1983.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.040s. Loss: 0.9345. top1: 84.33. top5: 98.73. : 100%|██████████| 157/157 [00:06<00:00, 24.74it/s]\n",
      "05/31/2020 04:04:55 - INFO - __main__ -   top-1 acc: 84.33\n",
      "05/31/2020 04:04:55 - INFO - __main__ -   top-5 acc: 98.73\n",
      "05/31/2020 04:04:55 - INFO - __main__ -   Best top-1 acc: 84.33\n",
      "05/31/2020 04:04:55 - INFO - __main__ -   Mean top-1 acc: 79.07\n",
      "\n",
      "Train Epoch: 28/1024. Iter:  203/1024. LR: 0.029980. Data: 0.043s. Batch: 0.660s. Loss: 0.2003. Loss_x: 0.0004. Loss_u: 0.1999. Mask: 0.8460. :  20%|█▉        | 203/1024 [02:13<08:15,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28/1024. Iter: 1024/1024. LR: 0.029979. Data: 0.032s. Batch: 0.642s. Loss: 0.1973. Loss_x: 0.0004. Loss_u: 0.1969. Mask: 0.8594. : 100%|██████████| 1024/1024 [10:57<00:00,  1.56it/s]\n",
      "05/31/2020 04:15:53 - INFO - __main__ -   Epoch 28. train_loss: 0.1973. train_loss_x: 0.0004. train_loss_u: 0.1969.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 0.8845. top1: 84.82. top5: 98.97. : 100%|██████████| 157/157 [00:06<00:00, 25.39it/s]\n",
      "05/31/2020 04:15:59 - INFO - __main__ -   top-1 acc: 84.82\n",
      "05/31/2020 04:15:59 - INFO - __main__ -   top-5 acc: 98.97\n",
      "05/31/2020 04:16:00 - INFO - __main__ -   Best top-1 acc: 84.82\n",
      "05/31/2020 04:16:00 - INFO - __main__ -   Mean top-1 acc: 79.79\n",
      "\n",
      "Train Epoch: 29/1024. Iter: 1024/1024. LR: 0.029977. Data: 0.030s. Batch: 0.623s. Loss: 0.1964. Loss_x: 0.0004. Loss_u: 0.1960. Mask: 0.8326. : 100%|██████████| 1024/1024 [10:38<00:00,  1.60it/s]\n",
      "05/31/2020 04:26:38 - INFO - __main__ -   Epoch 29. train_loss: 0.1964. train_loss_x: 0.0004. train_loss_u: 0.1960.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 0.8654. top1: 85.06. top5: 98.93. : 100%|██████████| 157/157 [00:06<00:00, 25.83it/s]\n",
      "05/31/2020 04:26:44 - INFO - __main__ -   top-1 acc: 85.06\n",
      "05/31/2020 04:26:44 - INFO - __main__ -   top-5 acc: 98.93\n",
      "05/31/2020 04:26:44 - INFO - __main__ -   Best top-1 acc: 85.06\n",
      "05/31/2020 04:26:44 - INFO - __main__ -   Mean top-1 acc: 80.43\n",
      "\n",
      "Train Epoch: 30/1024. Iter:  182/1024. LR: 0.029977. Data: 0.042s. Batch: 0.639s. Loss: 0.1942. Loss_x: 0.0003. Loss_u: 0.1939. Mask: 0.8683. :  18%|█▊        | 182/1024 [01:56<08:19,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30/1024. Iter: 1024/1024. LR: 0.029976. Data: 0.030s. Batch: 0.615s. Loss: 0.1951. Loss_x: 0.0003. Loss_u: 0.1948. Mask: 0.8549. : 100%|██████████| 1024/1024 [10:29<00:00,  1.63it/s]\n",
      "05/31/2020 04:37:14 - INFO - __main__ -   Epoch 30. train_loss: 0.1951. train_loss_x: 0.0003. train_loss_u: 0.1948.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.8523. top1: 85.36. top5: 98.93. : 100%|██████████| 157/157 [00:05<00:00, 26.95it/s]\n",
      "05/31/2020 04:37:20 - INFO - __main__ -   top-1 acc: 85.36\n",
      "05/31/2020 04:37:20 - INFO - __main__ -   top-5 acc: 98.93\n",
      "05/31/2020 04:37:20 - INFO - __main__ -   Best top-1 acc: 85.36\n",
      "05/31/2020 04:37:20 - INFO - __main__ -   Mean top-1 acc: 81.01\n",
      "\n",
      "Train Epoch: 31/1024. Iter: 1024/1024. LR: 0.029974. Data: 0.030s. Batch: 0.614s. Loss: 0.1943. Loss_x: 0.0003. Loss_u: 0.1940. Mask: 0.8594. : 100%|██████████| 1024/1024 [10:29<00:00,  1.63it/s]\n",
      "05/31/2020 04:47:49 - INFO - __main__ -   Epoch 31. train_loss: 0.1943. train_loss_x: 0.0003. train_loss_u: 0.1940.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.8367. top1: 85.54. top5: 98.96. : 100%|██████████| 157/157 [00:05<00:00, 26.20it/s]\n",
      "05/31/2020 04:47:55 - INFO - __main__ -   top-1 acc: 85.54\n",
      "05/31/2020 04:47:55 - INFO - __main__ -   top-5 acc: 98.96\n",
      "05/31/2020 04:47:55 - INFO - __main__ -   Best top-1 acc: 85.54\n",
      "05/31/2020 04:47:55 - INFO - __main__ -   Mean top-1 acc: 81.53\n",
      "\n",
      "Train Epoch: 32/1024. Iter:  338/1024. LR: 0.029973. Data: 0.034s. Batch: 0.622s. Loss: 0.1920. Loss_x: 0.0003. Loss_u: 0.1917. Mask: 0.8705. :  33%|███▎      | 338/1024 [03:30<06:48,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32/1024. Iter: 1024/1024. LR: 0.029972. Data: 0.029s. Batch: 0.614s. Loss: 0.1922. Loss_x: 0.0003. Loss_u: 0.1919. Mask: 0.8527. : 100%|██████████| 1024/1024 [10:28<00:00,  1.63it/s]\n",
      "05/31/2020 04:58:24 - INFO - __main__ -   Epoch 32. train_loss: 0.1922. train_loss_x: 0.0003. train_loss_u: 0.1919.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.8218. top1: 85.98. top5: 99.00. : 100%|██████████| 157/157 [00:05<00:00, 26.26it/s]\n",
      "05/31/2020 04:58:30 - INFO - __main__ -   top-1 acc: 85.98\n",
      "05/31/2020 04:58:30 - INFO - __main__ -   top-5 acc: 99.00\n",
      "05/31/2020 04:58:31 - INFO - __main__ -   Best top-1 acc: 85.98\n",
      "05/31/2020 04:58:31 - INFO - __main__ -   Mean top-1 acc: 82.05\n",
      "\n",
      "Train Epoch: 33/1024. Iter: 1024/1024. LR: 0.029971. Data: 0.030s. Batch: 0.613s. Loss: 0.1920. Loss_x: 0.0003. Loss_u: 0.1917. Mask: 0.8415. : 100%|██████████| 1024/1024 [10:27<00:00,  1.63it/s]\n",
      "05/31/2020 05:08:58 - INFO - __main__ -   Epoch 33. train_loss: 0.1920. train_loss_x: 0.0003. train_loss_u: 0.1917.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 0.7985. top1: 86.40. top5: 99.07. : 100%|██████████| 157/157 [00:06<00:00, 25.88it/s]\n",
      "05/31/2020 05:09:04 - INFO - __main__ -   top-1 acc: 86.40\n",
      "05/31/2020 05:09:04 - INFO - __main__ -   top-5 acc: 99.07\n",
      "05/31/2020 05:09:04 - INFO - __main__ -   Best top-1 acc: 86.40\n",
      "05/31/2020 05:09:04 - INFO - __main__ -   Mean top-1 acc: 82.53\n",
      "\n",
      "Train Epoch: 34/1024. Iter: 1024/1024. LR: 0.029969. Data: 0.029s. Batch: 0.618s. Loss: 0.1917. Loss_x: 0.0003. Loss_u: 0.1914. Mask: 0.8460. : 100%|██████████| 1024/1024 [10:32<00:00,  1.62it/s]\n",
      "05/31/2020 05:19:37 - INFO - __main__ -   Epoch 34. train_loss: 0.1917. train_loss_x: 0.0003. train_loss_u: 0.1914.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 0.7831. top1: 86.43. top5: 98.99. : 100%|██████████| 157/157 [00:05<00:00, 26.64it/s]\n",
      "05/31/2020 05:19:43 - INFO - __main__ -   top-1 acc: 86.43\n",
      "05/31/2020 05:19:43 - INFO - __main__ -   top-5 acc: 98.99\n",
      "05/31/2020 05:19:43 - INFO - __main__ -   Best top-1 acc: 86.43\n",
      "05/31/2020 05:19:43 - INFO - __main__ -   Mean top-1 acc: 82.97\n",
      "\n",
      "Train Epoch: 35/1024. Iter:  299/1024. LR: 0.029968. Data: 0.035s. Batch: 0.626s. Loss: 0.1906. Loss_x: 0.0003. Loss_u: 0.1903. Mask: 0.8817. :  29%|██▉       | 299/1024 [03:07<07:10,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35/1024. Iter: 1024/1024. LR: 0.029967. Data: 0.030s. Batch: 0.618s. Loss: 0.1900. Loss_x: 0.0003. Loss_u: 0.1898. Mask: 0.8549. : 100%|██████████| 1024/1024 [10:32<00:00,  1.62it/s]\n",
      "05/31/2020 05:30:16 - INFO - __main__ -   Epoch 35. train_loss: 0.1900. train_loss_x: 0.0003. train_loss_u: 0.1898.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.039s. Loss: 0.7707. top1: 86.78. top5: 99.05. : 100%|██████████| 157/157 [00:06<00:00, 25.46it/s]\n",
      "05/31/2020 05:30:22 - INFO - __main__ -   top-1 acc: 86.78\n",
      "05/31/2020 05:30:22 - INFO - __main__ -   top-5 acc: 99.05\n",
      "05/31/2020 05:30:22 - INFO - __main__ -   Best top-1 acc: 86.78\n",
      "05/31/2020 05:30:22 - INFO - __main__ -   Mean top-1 acc: 83.39\n",
      "\n",
      "Train Epoch: 36/1024. Iter: 1024/1024. LR: 0.029965. Data: 0.030s. Batch: 0.622s. Loss: 0.1889. Loss_x: 0.0003. Loss_u: 0.1886. Mask: 0.8795. : 100%|██████████| 1024/1024 [10:36<00:00,  1.61it/s]\n",
      "05/31/2020 05:40:59 - INFO - __main__ -   Epoch 36. train_loss: 0.1889. train_loss_x: 0.0003. train_loss_u: 0.1886.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.7531. top1: 87.15. top5: 99.10. : 100%|██████████| 157/157 [00:06<00:00, 25.90it/s]\n",
      "05/31/2020 05:41:05 - INFO - __main__ -   top-1 acc: 87.15\n",
      "05/31/2020 05:41:05 - INFO - __main__ -   top-5 acc: 99.10\n",
      "05/31/2020 05:41:05 - INFO - __main__ -   Best top-1 acc: 87.15\n",
      "05/31/2020 05:41:05 - INFO - __main__ -   Mean top-1 acc: 83.79\n",
      "\n",
      "Train Epoch: 37/1024. Iter:  464/1024. LR: 0.029964. Data: 0.033s. Batch: 0.626s. Loss: 0.1878. Loss_x: 0.0003. Loss_u: 0.1876. Mask: 0.8839. :  45%|████▌     | 464/1024 [04:50<05:32,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37/1024. Iter: 1024/1024. LR: 0.029963. Data: 0.030s. Batch: 0.620s. Loss: 0.1872. Loss_x: 0.0003. Loss_u: 0.1869. Mask: 0.8616. : 100%|██████████| 1024/1024 [10:34<00:00,  1.61it/s]\n",
      "05/31/2020 05:51:40 - INFO - __main__ -   Epoch 37. train_loss: 0.1872. train_loss_x: 0.0003. train_loss_u: 0.1869.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.7603. top1: 87.17. top5: 99.07. : 100%|██████████| 157/157 [00:06<00:00, 25.81it/s]\n",
      "05/31/2020 05:51:46 - INFO - __main__ -   top-1 acc: 87.17\n",
      "05/31/2020 05:51:46 - INFO - __main__ -   top-5 acc: 99.07\n",
      "05/31/2020 05:51:46 - INFO - __main__ -   Best top-1 acc: 87.17\n",
      "05/31/2020 05:51:46 - INFO - __main__ -   Mean top-1 acc: 84.18\n",
      "\n",
      "Train Epoch: 38/1024. Iter: 1024/1024. LR: 0.029961. Data: 0.029s. Batch: 0.619s. Loss: 0.1870. Loss_x: 0.0003. Loss_u: 0.1867. Mask: 0.8795. : 100%|██████████| 1024/1024 [10:34<00:00,  1.61it/s]\n",
      "05/31/2020 06:02:21 - INFO - __main__ -   Epoch 38. train_loss: 0.1870. train_loss_x: 0.0003. train_loss_u: 0.1867.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 0.7555. top1: 87.21. top5: 99.13. : 100%|██████████| 157/157 [00:05<00:00, 26.36it/s]\n",
      "05/31/2020 06:02:27 - INFO - __main__ -   top-1 acc: 87.21\n",
      "05/31/2020 06:02:27 - INFO - __main__ -   top-5 acc: 99.13\n",
      "05/31/2020 06:02:27 - INFO - __main__ -   Best top-1 acc: 87.21\n",
      "05/31/2020 06:02:27 - INFO - __main__ -   Mean top-1 acc: 84.56\n",
      "\n",
      "Train Epoch: 39/1024. Iter: 1003/1024. LR: 0.029959. Data: 0.029s. Batch: 0.617s. Loss: 0.1859. Loss_x: 0.0003. Loss_u: 0.1856. Mask: 0.8772. :  98%|█████████▊| 1003/1024 [10:18<00:12,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 39/1024. Iter: 1024/1024. LR: 0.029959. Data: 0.029s. Batch: 0.616s. Loss: 0.1860. Loss_x: 0.0003. Loss_u: 0.1857. Mask: 0.8705. : 100%|██████████| 1024/1024 [10:30<00:00,  1.62it/s]\n",
      "05/31/2020 06:12:57 - INFO - __main__ -   Epoch 39. train_loss: 0.1860. train_loss_x: 0.0003. train_loss_u: 0.1857.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.7662. top1: 87.51. top5: 99.21. : 100%|██████████| 157/157 [00:06<00:00, 25.65it/s]\n",
      "05/31/2020 06:13:03 - INFO - __main__ -   top-1 acc: 87.51\n",
      "05/31/2020 06:13:03 - INFO - __main__ -   top-5 acc: 99.21\n",
      "05/31/2020 06:13:03 - INFO - __main__ -   Best top-1 acc: 87.51\n",
      "05/31/2020 06:13:03 - INFO - __main__ -   Mean top-1 acc: 84.91\n",
      "\n",
      "Train Epoch: 40/1024. Iter: 1024/1024. LR: 0.029957. Data: 0.029s. Batch: 0.609s. Loss: 0.1856. Loss_x: 0.0003. Loss_u: 0.1853. Mask: 0.8795. : 100%|██████████| 1024/1024 [10:24<00:00,  1.64it/s]\n",
      "05/31/2020 06:23:28 - INFO - __main__ -   Epoch 40. train_loss: 0.1856. train_loss_x: 0.0003. train_loss_u: 0.1853.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.7431. top1: 87.59. top5: 99.17. : 100%|██████████| 157/157 [00:06<00:00, 25.94it/s]\n",
      "05/31/2020 06:23:34 - INFO - __main__ -   top-1 acc: 87.59\n",
      "05/31/2020 06:23:34 - INFO - __main__ -   top-5 acc: 99.17\n",
      "05/31/2020 06:23:34 - INFO - __main__ -   Best top-1 acc: 87.59\n",
      "05/31/2020 06:23:34 - INFO - __main__ -   Mean top-1 acc: 85.23\n",
      "\n",
      "Train Epoch: 41/1024. Iter: 1024/1024. LR: 0.029955. Data: 0.029s. Batch: 0.612s. Loss: 0.1825. Loss_x: 0.0002. Loss_u: 0.1822. Mask: 0.8705. : 100%|██████████| 1024/1024 [10:27<00:00,  1.63it/s]\n",
      "05/31/2020 06:34:01 - INFO - __main__ -   Epoch 41. train_loss: 0.1825. train_loss_x: 0.0002. train_loss_u: 0.1822.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 0.7236. top1: 87.61. top5: 99.26. : 100%|██████████| 157/157 [00:05<00:00, 26.22it/s]\n",
      "05/31/2020 06:34:07 - INFO - __main__ -   top-1 acc: 87.61\n",
      "05/31/2020 06:34:07 - INFO - __main__ -   top-5 acc: 99.26\n",
      "05/31/2020 06:34:07 - INFO - __main__ -   Best top-1 acc: 87.61\n",
      "05/31/2020 06:34:07 - INFO - __main__ -   Mean top-1 acc: 85.53\n",
      "\n",
      "Train Epoch: 42/1024. Iter:  103/1024. LR: 0.029954. Data: 0.051s. Batch: 0.651s. Loss: 0.1874. Loss_x: 0.0002. Loss_u: 0.1872. Mask: 0.9018. :  10%|█         | 103/1024 [01:07<08:51,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42/1024. Iter:  875/1024. LR: 0.029953. Data: 0.029s. Batch: 0.614s. Loss: 0.1845. Loss_x: 0.0003. Loss_u: 0.1842. Mask: 0.8661. :  85%|████████▌ | 875/1024 [08:56<01:26,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 42/1024. Iter: 1024/1024. LR: 0.029952. Data: 0.029s. Batch: 0.611s. Loss: 0.1850. Loss_x: 0.0003. Loss_u: 0.1847. Mask: 0.8951. : 100%|██████████| 1024/1024 [10:25<00:00,  1.64it/s]\n",
      "05/31/2020 06:44:33 - INFO - __main__ -   Epoch 42. train_loss: 0.1850. train_loss_x: 0.0003. train_loss_u: 0.1847.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.7066. top1: 87.72. top5: 99.24. : 100%|██████████| 157/157 [00:05<00:00, 26.92it/s]\n",
      "05/31/2020 06:44:39 - INFO - __main__ -   top-1 acc: 87.72\n",
      "05/31/2020 06:44:39 - INFO - __main__ -   top-5 acc: 99.24\n",
      "05/31/2020 06:44:39 - INFO - __main__ -   Best top-1 acc: 87.72\n",
      "05/31/2020 06:44:39 - INFO - __main__ -   Mean top-1 acc: 85.80\n",
      "\n",
      "Train Epoch: 43/1024. Iter: 1024/1024. LR: 0.029950. Data: 0.030s. Batch: 0.609s. Loss: 0.1821. Loss_x: 0.0002. Loss_u: 0.1819. Mask: 0.8862. : 100%|██████████| 1024/1024 [10:24<00:00,  1.64it/s]\n",
      "05/31/2020 06:55:03 - INFO - __main__ -   Epoch 43. train_loss: 0.1821. train_loss_x: 0.0002. train_loss_u: 0.1819.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6856. top1: 88.04. top5: 99.40. : 100%|██████████| 157/157 [00:05<00:00, 26.70it/s]\n",
      "05/31/2020 06:55:09 - INFO - __main__ -   top-1 acc: 88.04\n",
      "05/31/2020 06:55:09 - INFO - __main__ -   top-5 acc: 99.40\n",
      "05/31/2020 06:55:09 - INFO - __main__ -   Best top-1 acc: 88.04\n",
      "05/31/2020 06:55:09 - INFO - __main__ -   Mean top-1 acc: 86.06\n",
      "\n",
      "Train Epoch: 44/1024. Iter: 1024/1024. LR: 0.029948. Data: 0.029s. Batch: 0.613s. Loss: 0.1823. Loss_x: 0.0002. Loss_u: 0.1821. Mask: 0.8973. : 100%|██████████| 1024/1024 [10:27<00:00,  1.63it/s]\n",
      "05/31/2020 07:05:37 - INFO - __main__ -   Epoch 44. train_loss: 0.1823. train_loss_x: 0.0002. train_loss_u: 0.1821.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6981. top1: 88.16. top5: 99.33. : 100%|██████████| 157/157 [00:05<00:00, 26.59it/s]\n",
      "05/31/2020 07:05:43 - INFO - __main__ -   top-1 acc: 88.16\n",
      "05/31/2020 07:05:43 - INFO - __main__ -   top-5 acc: 99.33\n",
      "05/31/2020 07:05:43 - INFO - __main__ -   Best top-1 acc: 88.16\n",
      "05/31/2020 07:05:43 - INFO - __main__ -   Mean top-1 acc: 86.31\n",
      "\n",
      "Train Epoch: 45/1024. Iter: 1024/1024. LR: 0.029945. Data: 0.029s. Batch: 0.612s. Loss: 0.1817. Loss_x: 0.0002. Loss_u: 0.1815. Mask: 0.9085. : 100%|██████████| 1024/1024 [10:26<00:00,  1.63it/s]\n",
      "05/31/2020 07:16:10 - INFO - __main__ -   Epoch 45. train_loss: 0.1817. train_loss_x: 0.0002. train_loss_u: 0.1815.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.7104. top1: 88.11. top5: 99.32. : 100%|██████████| 157/157 [00:05<00:00, 26.53it/s]\n",
      "05/31/2020 07:16:15 - INFO - __main__ -   top-1 acc: 88.11\n",
      "05/31/2020 07:16:15 - INFO - __main__ -   top-5 acc: 99.32\n",
      "05/31/2020 07:16:16 - INFO - __main__ -   Best top-1 acc: 88.16\n",
      "05/31/2020 07:16:16 - INFO - __main__ -   Mean top-1 acc: 86.54\n",
      "\n",
      "Train Epoch: 46/1024. Iter: 1024/1024. LR: 0.029943. Data: 0.029s. Batch: 0.610s. Loss: 0.1805. Loss_x: 0.0002. Loss_u: 0.1802. Mask: 0.9018. : 100%|██████████| 1024/1024 [10:25<00:00,  1.64it/s]\n",
      "05/31/2020 07:26:41 - INFO - __main__ -   Epoch 46. train_loss: 0.1805. train_loss_x: 0.0002. train_loss_u: 0.1802.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 0.7017. top1: 88.37. top5: 99.38. : 100%|██████████| 157/157 [00:05<00:00, 26.22it/s]\n",
      "05/31/2020 07:26:47 - INFO - __main__ -   top-1 acc: 88.37\n",
      "05/31/2020 07:26:47 - INFO - __main__ -   top-5 acc: 99.38\n",
      "05/31/2020 07:26:47 - INFO - __main__ -   Best top-1 acc: 88.37\n",
      "05/31/2020 07:26:47 - INFO - __main__ -   Mean top-1 acc: 86.77\n",
      "\n",
      "Train Epoch: 47/1024. Iter:  470/1024. LR: 0.029942. Data: 0.031s. Batch: 0.615s. Loss: 0.1809. Loss_x: 0.0002. Loss_u: 0.1807. Mask: 0.8906. :  46%|████▌     | 470/1024 [04:48<05:29,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47/1024. Iter: 1024/1024. LR: 0.029940. Data: 0.029s. Batch: 0.613s. Loss: 0.1804. Loss_x: 0.0002. Loss_u: 0.1802. Mask: 0.8951. : 100%|██████████| 1024/1024 [10:28<00:00,  1.63it/s]\n",
      "05/31/2020 07:37:15 - INFO - __main__ -   Epoch 47. train_loss: 0.1804. train_loss_x: 0.0002. train_loss_u: 0.1802.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6956. top1: 88.44. top5: 99.33. : 100%|██████████| 157/157 [00:05<00:00, 26.42it/s]\n",
      "05/31/2020 07:37:21 - INFO - __main__ -   top-1 acc: 88.44\n",
      "05/31/2020 07:37:21 - INFO - __main__ -   top-5 acc: 99.33\n",
      "05/31/2020 07:37:21 - INFO - __main__ -   Best top-1 acc: 88.44\n",
      "05/31/2020 07:37:21 - INFO - __main__ -   Mean top-1 acc: 86.97\n",
      "\n",
      "Train Epoch: 48/1024. Iter: 1024/1024. LR: 0.029938. Data: 0.030s. Batch: 0.612s. Loss: 0.1801. Loss_x: 0.0002. Loss_u: 0.1799. Mask: 0.8906. : 100%|██████████| 1024/1024 [10:26<00:00,  1.63it/s]\n",
      "05/31/2020 07:47:48 - INFO - __main__ -   Epoch 48. train_loss: 0.1801. train_loss_x: 0.0002. train_loss_u: 0.1799.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6924. top1: 88.53. top5: 99.35. : 100%|██████████| 157/157 [00:05<00:00, 26.41it/s]\n",
      "05/31/2020 07:47:54 - INFO - __main__ -   top-1 acc: 88.53\n",
      "05/31/2020 07:47:54 - INFO - __main__ -   top-5 acc: 99.35\n",
      "05/31/2020 07:47:54 - INFO - __main__ -   Best top-1 acc: 88.53\n",
      "05/31/2020 07:47:54 - INFO - __main__ -   Mean top-1 acc: 87.16\n",
      "\n",
      "Train Epoch: 49/1024. Iter:  518/1024. LR: 0.029936. Data: 0.032s. Batch: 0.614s. Loss: 0.1754. Loss_x: 0.0002. Loss_u: 0.1752. Mask: 0.9129. :  51%|█████     | 518/1024 [05:17<04:55,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/1024. Iter: 1024/1024. LR: 0.029935. Data: 0.029s. Batch: 0.610s. Loss: 0.1769. Loss_x: 0.0002. Loss_u: 0.1768. Mask: 0.8862. : 100%|██████████| 1024/1024 [10:24<00:00,  1.64it/s]\n",
      "05/31/2020 07:58:19 - INFO - __main__ -   Epoch 49. train_loss: 0.1769. train_loss_x: 0.0002. train_loss_u: 0.1768.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6999. top1: 88.62. top5: 99.34. : 100%|██████████| 157/157 [00:05<00:00, 26.67it/s]\n",
      "05/31/2020 07:58:25 - INFO - __main__ -   top-1 acc: 88.62\n",
      "05/31/2020 07:58:25 - INFO - __main__ -   top-5 acc: 99.34\n",
      "05/31/2020 07:58:25 - INFO - __main__ -   Best top-1 acc: 88.62\n",
      "05/31/2020 07:58:25 - INFO - __main__ -   Mean top-1 acc: 87.34\n",
      "\n",
      "Train Epoch: 50/1024. Iter:  775/1024. LR: 0.029933. Data: 0.030s. Batch: 0.612s. Loss: 0.1780. Loss_x: 0.0002. Loss_u: 0.1778. Mask: 0.9063. :  76%|███████▌  | 775/1024 [07:54<02:27,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50/1024. Iter: 1024/1024. LR: 0.029932. Data: 0.029s. Batch: 0.610s. Loss: 0.1783. Loss_x: 0.0002. Loss_u: 0.1782. Mask: 0.9129. : 100%|██████████| 1024/1024 [10:24<00:00,  1.64it/s]\n",
      "05/31/2020 08:08:49 - INFO - __main__ -   Epoch 50. train_loss: 0.1783. train_loss_x: 0.0002. train_loss_u: 0.1782.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.037s. Loss: 0.6831. top1: 88.72. top5: 99.43. : 100%|██████████| 157/157 [00:05<00:00, 26.67it/s]\n",
      "05/31/2020 08:08:55 - INFO - __main__ -   top-1 acc: 88.72\n",
      "05/31/2020 08:08:55 - INFO - __main__ -   top-5 acc: 99.43\n",
      "05/31/2020 08:08:55 - INFO - __main__ -   Best top-1 acc: 88.72\n",
      "05/31/2020 08:08:55 - INFO - __main__ -   Mean top-1 acc: 87.50\n",
      "\n",
      "Train Epoch: 51/1024. Iter: 1024/1024. LR: 0.029930. Data: 0.029s. Batch: 0.613s. Loss: 0.1753. Loss_x: 0.0002. Loss_u: 0.1751. Mask: 0.9174. : 100%|██████████| 1024/1024 [10:27<00:00,  1.63it/s]\n",
      "05/31/2020 08:19:23 - INFO - __main__ -   Epoch 51. train_loss: 0.1753. train_loss_x: 0.0002. train_loss_u: 0.1751.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.038s. Loss: 0.6668. top1: 88.93. top5: 99.47. : 100%|██████████| 157/157 [00:06<00:00, 26.07it/s]\n",
      "05/31/2020 08:19:29 - INFO - __main__ -   top-1 acc: 88.93\n",
      "05/31/2020 08:19:29 - INFO - __main__ -   top-5 acc: 99.47\n",
      "05/31/2020 08:19:29 - INFO - __main__ -   Best top-1 acc: 88.93\n",
      "05/31/2020 08:19:29 - INFO - __main__ -   Mean top-1 acc: 87.67\n",
      "\n",
      "Train Epoch: 52/1024. Iter: 1024/1024. LR: 0.029927. Data: 0.029s. Batch: 0.611s. Loss: 0.1754. Loss_x: 0.0002. Loss_u: 0.1753. Mask: 0.9263. : 100%|██████████| 1024/1024 [10:25<00:00,  1.64it/s]\n",
      "05/31/2020 08:29:55 - INFO - __main__ -   Epoch 52. train_loss: 0.1754. train_loss_x: 0.0002. train_loss_u: 0.1753.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.6730. top1: 89.00. top5: 99.40. : 100%|██████████| 157/157 [00:05<00:00, 26.29it/s]\n",
      "05/31/2020 08:30:01 - INFO - __main__ -   top-1 acc: 89.00\n",
      "05/31/2020 08:30:01 - INFO - __main__ -   top-5 acc: 99.40\n",
      "05/31/2020 08:30:01 - INFO - __main__ -   Best top-1 acc: 89.00\n",
      "05/31/2020 08:30:01 - INFO - __main__ -   Mean top-1 acc: 87.82\n",
      "\n",
      "Train Epoch: 53/1024. Iter: 1024/1024. LR: 0.029924. Data: 0.029s. Batch: 0.610s. Loss: 0.1757. Loss_x: 0.0002. Loss_u: 0.1755. Mask: 0.9308. : 100%|██████████| 1024/1024 [10:24<00:00,  1.64it/s]\n",
      "05/31/2020 08:40:25 - INFO - __main__ -   Epoch 53. train_loss: 0.1757. train_loss_x: 0.0002. train_loss_u: 0.1755.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 0.6953. top1: 88.97. top5: 99.38. : 100%|██████████| 157/157 [00:05<00:00, 26.54it/s]\n",
      "05/31/2020 08:40:31 - INFO - __main__ -   top-1 acc: 88.97\n",
      "05/31/2020 08:40:31 - INFO - __main__ -   top-5 acc: 99.38\n",
      "05/31/2020 08:40:31 - INFO - __main__ -   Best top-1 acc: 89.00\n",
      "05/31/2020 08:40:31 - INFO - __main__ -   Mean top-1 acc: 87.95\n",
      "\n",
      "Train Epoch: 54/1024. Iter: 1024/1024. LR: 0.029921. Data: 0.028s. Batch: 0.609s. Loss: 0.1734. Loss_x: 0.0002. Loss_u: 0.1732. Mask: 0.9308. : 100%|██████████| 1024/1024 [10:23<00:00,  1.64it/s]\n",
      "05/31/2020 08:50:55 - INFO - __main__ -   Epoch 54. train_loss: 0.1734. train_loss_x: 0.0002. train_loss_u: 0.1732.\n",
      "Test Iter:  157/ 157. Data: 0.008s. Batch: 0.038s. Loss: 0.7006. top1: 88.94. top5: 99.36. : 100%|██████████| 157/157 [00:05<00:00, 26.25it/s]\n",
      "05/31/2020 08:51:01 - INFO - __main__ -   top-1 acc: 88.94\n",
      "05/31/2020 08:51:01 - INFO - __main__ -   top-5 acc: 99.36\n",
      "05/31/2020 08:51:01 - INFO - __main__ -   Best top-1 acc: 89.00\n",
      "05/31/2020 08:51:01 - INFO - __main__ -   Mean top-1 acc: 88.08\n",
      "\n",
      "Train Epoch: 55/1024. Iter:  179/1024. LR: 0.029921. Data: 0.039s. Batch: 0.623s. Loss: 0.1728. Loss_x: 0.0001. Loss_u: 0.1727. Mask: 0.9063. :  17%|█▋        | 179/1024 [01:51<08:06,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 55/1024. Iter: 1024/1024. LR: 0.029918. Data: 0.029s. Batch: 0.607s. Loss: 0.1742. Loss_x: 0.0001. Loss_u: 0.1740. Mask: 0.9219. : 100%|██████████| 1024/1024 [10:21<00:00,  1.65it/s]\n",
      "05/31/2020 09:01:23 - INFO - __main__ -   Epoch 55. train_loss: 0.1742. train_loss_x: 0.0001. train_loss_u: 0.1740.\n",
      "Test Iter:  157/ 157. Data: 0.007s. Batch: 0.037s. Loss: 0.6997. top1: 88.88. top5: 99.33. : 100%|██████████| 157/157 [00:05<00:00, 26.76it/s]\n",
      "05/31/2020 09:01:29 - INFO - __main__ -   top-1 acc: 88.88\n",
      "05/31/2020 09:01:29 - INFO - __main__ -   top-5 acc: 99.33\n",
      "05/31/2020 09:01:29 - INFO - __main__ -   Best top-1 acc: 89.00\n",
      "05/31/2020 09:01:29 - INFO - __main__ -   Mean top-1 acc: 88.18\n",
      "\n",
      "Train Epoch: 56/1024. Iter:  927/1024. LR: 0.029916. Data: 0.030s. Batch: 0.611s. Loss: 0.1734. Loss_x: 0.0002. Loss_u: 0.1732. Mask: 0.9286. :  91%|█████████ | 927/1024 [09:26<00:58,  1.66it/s]"
     ]
    }
   ],
   "source": [
    "test_accs = []\n",
    "model.zero_grad()\n",
    "for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "    train_loss, train_loss_x, train_loss_u, mask_prob = train_one_epoch(args, labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_model, scheduler, epoch)\n",
    "\n",
    "    logger.info(\"Epoch {}. train_loss: {:.4f}. train_loss_x: {:.4f}. train_loss_u: {:.4f}.\".format(epoch+1, train_loss, train_loss_x, train_loss_u))\n",
    "\n",
    "    if args.use_ema:\n",
    "        test_model = ema_model.ema\n",
    "    else:\n",
    "        test_model = model\n",
    "\n",
    "    test_loss, test_acc = test(args, test_loader, test_model, epoch)\n",
    "\n",
    "    is_best = test_acc > best_acc\n",
    "    best_acc = max(test_acc, best_acc)\n",
    "    model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "    if args.use_ema:\n",
    "        ema_to_save = ema_model.ema.module if hasattr(ema_model.ema, \"module\") else ema_model.ema\n",
    "    save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model_to_save.state_dict(),\n",
    "                'ema_state_dict': ema_to_save.state_dict() if args.use_ema else None,\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, args.out)\n",
    "\n",
    "    test_accs.append(test_acc)\n",
    "    logger.info('Best top-1 acc: {:.2f}'.format(best_acc))\n",
    "    logger.info('Mean top-1 acc: {:.2f}\\n'.format(np.mean(test_accs[-20:])))\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FixMatch Training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1bcde06d6d254ff4bc17a7f149034526": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7c9d4fc4aa824d30b2a3fdf62b571b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6942dbb2bc147e19607ccdcf691abf5",
      "placeholder": "​",
      "style": "IPY_MODEL_cde28a322daf464797613453994a4215",
      "value": " 170500096/? [00:30&lt;00:00, 16092508.46it/s]"
     }
    },
    "89a8c7f7b2204c1abc4dca984ae9a554": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e1508db0d94b45718e593c1f85ed7284",
       "IPY_MODEL_7c9d4fc4aa824d30b2a3fdf62b571b68"
      ],
      "layout": "IPY_MODEL_9f4140ed63f0401f88522b03dff1a1d5"
     }
    },
    "89bbc7041786423e9b6cd9b0a1d647f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4140ed63f0401f88522b03dff1a1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cde28a322daf464797613453994a4215": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1508db0d94b45718e593c1f85ed7284": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89bbc7041786423e9b6cd9b0a1d647f0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1bcde06d6d254ff4bc17a7f149034526",
      "value": 1
     }
    },
    "f6942dbb2bc147e19607ccdcf691abf5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
